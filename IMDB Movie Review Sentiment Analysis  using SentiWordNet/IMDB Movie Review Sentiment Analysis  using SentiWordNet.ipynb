{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Mini Project - Group A7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a577dade9pso"
      },
      "source": [
        "#***NLP Mini Project : IMDB Movie Review Sentiment Analysis using SentiWordNet***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzRBf_gc_I5R"
      },
      "source": [
        "#Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yd58y-I_LaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f38747-525e-487d-9dd8-133524c936ba"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "\n",
        "nltk.download('all', halt_on_error=False)\n",
        "tokenizer=ToktokTokenizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCsk-RZJ-zgi"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHqfWj3tLwGU",
        "outputId": "d7179313-d0b6-4a10-c221-ef2956589ef0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEz0kf0q-1fs"
      },
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')\n",
        "\n",
        "reviews = np.array(dataset['review'])\n",
        "sentiments = np.array(dataset['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPtkQCE9OsLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9eac11-8398-46a1-c69c-5fe98a29e316"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "total_df=pd.DataFrame({\"review\":reviews,\"sentiment\":sentiments})\n",
        "total_df=shuffle(total_df)\n",
        "total_df=total_df.reset_index(drop=True)\n",
        "print(total_df.head())\n",
        "\n",
        "print(len(total_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              review sentiment\n",
            "0  A clever overall story/location for a story. A...  negative\n",
            "1  Dear Readers,<br /><br />The final battle betw...  positive\n",
            "2  The movie eXistenZ is about a futuristic video...  positive\n",
            "3  \"Alexander Nevsky\" marked director Sergei Eise...  positive\n",
            "4  Mockumentaries are proliferating lately so muc...  negative\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YatyIfo-oCY"
      },
      "source": [
        "#Cleaning the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEF8dzpjF1KN"
      },
      "source": [
        "Removing HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT4pDd0V9odr"
      },
      "source": [
        "def strip_html_tag(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS9AVyI1F9wi"
      },
      "source": [
        "Remove Accented Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S88swZpF7Iy"
      },
      "source": [
        "def strip_accents(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzwQ-_WrGB1w"
      },
      "source": [
        "Expanding Contraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKmtGp9VGDd4"
      },
      "source": [
        "CONTRACTION_MAP = {\"ain't\": \"is not\",\n",
        "                   \"aren't\": \"are not\",\n",
        "                   \"can't\": \"cannot\",\n",
        "                   \"can't've\": \"cannot have\",\n",
        "                   \"'cause\": \"because\",\n",
        "                   \"could've\": \"could have\",\n",
        "                   \"couldn't\": \"could not\",\n",
        "                   \"couldn't've\": \"could not have\",\n",
        "                   \"didn't\": \"did not\",\n",
        "                   \"doesn't\": \"does not\",\n",
        "                   \"don't\": \"do not\",\n",
        "                   \"hadn't\": \"had not\",\n",
        "                   \"hadn't've\": \"had not have\",\n",
        "                   \"hasn't\": \"has not\",\n",
        "                   \"haven't\": \"have not\",\n",
        "                   \"he'd\": \"he would\",\n",
        "                   \"he'd've\": \"he would have\",\n",
        "                   \"he'll\": \"he will\",\n",
        "                   \"he'll've\": \"he will have\",\n",
        "                   \"he's\": \"he is\",\n",
        "                   \"how'd\": \"how did\",\n",
        "                   \"how'd'y\": \"how do you\",\n",
        "                   \"how'll\": \"how will\",\n",
        "                   \"how's\": \"how is\",\n",
        "                   \"I'd\": \"I would\",\n",
        "                   \"I'd've\": \"I would have\",\n",
        "                   \"I'll\": \"I will\",\n",
        "                   \"I'll've\": \"I will have\",\n",
        "                   \"I'm\": \"I am\",\n",
        "                   \"I've\": \"I have\",\n",
        "                   \"i'd\": \"i would\",\n",
        "                   \"i'd've\": \"i would have\",\n",
        "                   \"i'll\": \"i will\",\n",
        "                   \"i'll've\": \"i will have\",\n",
        "                   \"i'm\": \"i am\",\n",
        "                   \"i've\": \"i have\",\n",
        "                   \"isn't\": \"is not\",\n",
        "                   \"it'd\": \"it would\",\n",
        "                   \"it'd've\": \"it would have\",\n",
        "                   \"it'll\": \"it will\",\n",
        "                   \"it'll've\": \"it will have\",\n",
        "                   \"it's\": \"it is\",\n",
        "                   \"let's\": \"let us\",\n",
        "                   \"ma'am\": \"madam\",\n",
        "                   \"mayn't\": \"may not\",\n",
        "                   \"might've\": \"might have\",\n",
        "                   \"mightn't\": \"might not\",\n",
        "                   \"mightn't've\": \"might not have\",\n",
        "                   \"must've\": \"must have\",\n",
        "                   \"mustn't\": \"must not\",\n",
        "                   \"mustn't've\": \"must not have\",\n",
        "                   \"needn't\": \"need not\",\n",
        "                   \"needn't've\": \"need not have\",\n",
        "                   \"o'clock\": \"of the clock\",\n",
        "                   \"oughtn't\": \"ought not\",\n",
        "                   \"oughtn't've\": \"ought not have\",\n",
        "                   \"shan't\": \"shall not\",\n",
        "                   \"sha'n't\": \"shall not\",\n",
        "                   \"shan't've\": \"shall not have\",\n",
        "                   \"she'd\": \"she would\",\n",
        "                   \"she'd've\": \"she would have\",\n",
        "                   \"she'll\": \"she will\",\n",
        "                   \"she'll've\": \"she will have\",\n",
        "                   \"she's\": \"she is\",\n",
        "                   \"should've\": \"should have\",\n",
        "                   \"shouldn't\": \"should not\",\n",
        "                   \"shouldn't've\": \"should not have\",\n",
        "                   \"so've\": \"so have\",\n",
        "                   \"so's\": \"so as\",\n",
        "                   \"that'd\": \"that would\",\n",
        "                   \"that'd've\": \"that would have\",\n",
        "                   \"that's\": \"that is\",\n",
        "                   \"there'd\": \"there would\",\n",
        "                   \"there'd've\": \"there would have\",\n",
        "                   \"there's\": \"there is\",\n",
        "                   \"they'd\": \"they would\",\n",
        "                   \"they'd've\": \"they would have\",\n",
        "                   \"they'll\": \"they will\",\n",
        "                   \"they'll've\": \"they will have\",\n",
        "                   \"they're\": \"they are\",\n",
        "                   \"they've\": \"they have\",\"to've\": \"to have\",\n",
        "                   \"wasn't\": \"was not\",\n",
        "                   \"we'd\": \"we would\",\n",
        "                   \"we'd've\": \"we would have\",\n",
        "                   \"we'll\": \"we will\",\n",
        "                   \"we'll've\": \"we will have\",\n",
        "                   \"we're\": \"we are\",\n",
        "                   \"we've\": \"we have\",\n",
        "                   \"weren't\": \"were not\",\n",
        "                   \"what'll\": \"what will\",\n",
        "                   \"what'll've\": \"what will have\",\n",
        "                   \"what're\": \"what are\",\n",
        "                   \"what's\": \"what is\",\n",
        "                   \"what've\": \"what have\",\n",
        "                   \"when's\": \"when is\",\n",
        "                   \"when've\": \"when have\",\n",
        "                   \"where'd\": \"where did\",\n",
        "                   \"where's\": \"where is\",\n",
        "                   \"where've\": \"where have\",\n",
        "                   \"who'll\": \"who will\",\n",
        "                   \"who'll've\": \"who will have\",\n",
        "                   \"who's\": \"who is\",\n",
        "                   \"who've\": \"who have\",\n",
        "                   \"why's\": \"why is\",\n",
        "                   \"why've\": \"why have\",\n",
        "                   \"will've\": \"will have\",\n",
        "                   \"won't\": \"will not\",\n",
        "                   \"won't've\": \"will not have\",\n",
        "                   \"would've\": \"would have\",\n",
        "                   \"wouldn't\": \"would not\",\n",
        "                   \"wouldn't've\": \"would not have\",\n",
        "                   \"y'all\": \"you all\",\n",
        "                   \"y'all'd\": \"you all would\",\n",
        "                   \"y'all'd've\": \"you all would have\",\n",
        "                   \"y'all're\": \"you all are\",\n",
        "                   \"y'all've\": \"you all have\",\n",
        "                   \"you'd\": \"you would\",\n",
        "                   \"you'd've\": \"you would have\",\n",
        "                   \"you'll\": \"you will\",\n",
        "                   \"you'll've\": \"you will have\",\n",
        "                   \"you're\": \"you are\",\n",
        "                   \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6uMsMB8GKEP"
      },
      "source": [
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "z_ZbuFBhGNJd",
        "outputId": "50c20517-e24a-4760-9197-25972e038df1"
      },
      "source": [
        "#Trying with example\n",
        "expand_contractions(\"It's an amazing language which can be used for Scripting\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'It is an amazing language which can be used for Scripting'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7wXmT6FGaoU"
      },
      "source": [
        "Removing Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38LW1qQUGcek"
      },
      "source": [
        "def strip_special_characters(text):\n",
        "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrxMxVYQGfNL"
      },
      "source": [
        "Lemmatizing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4GXKkD9GktT"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIr25Ce2Gnb0"
      },
      "source": [
        "def lemmatize_text(text):\n",
        "    tokens = nltk.word_tokenize (text)\n",
        "    text =' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbe67wYSGwBb"
      },
      "source": [
        "Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioj9Wu0QGxU3"
      },
      "source": [
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDAPJ5DKIPSH"
      },
      "source": [
        "def strip_stopwords(text, is_lower_case=False):\n",
        "    tokens = nltk.word_tokenize (text)\n",
        "    #tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8EwfKdMIVNB"
      },
      "source": [
        "Cleaned Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_fsYBDvIZLd"
      },
      "source": [
        "def clean_text(text,strip_html=True, expand_contraction=True,\n",
        "               accent_remove=True, text_lower_case=True,text_lemmatize=True, \n",
        "               special_char_remove=True, stopword_remove=True):\n",
        "    \n",
        "    processed_text=[]\n",
        "    for doc in text:\n",
        "        \n",
        "        #HTML tah striping\n",
        "        if strip_html:\n",
        "            doc=strip_html_tag(doc)\n",
        "        \n",
        "        ## remove accented characters\n",
        "        if accent_remove:\n",
        "            doc = strip_accents(doc)\n",
        "            \n",
        "        # expand contractions    \n",
        "        if expand_contraction:\n",
        "            doc = expand_contractions(doc)\n",
        "            \n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "            \n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        \n",
        "        # insert spaces between special characters to isolate them    \n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "        \n",
        "        # lemmatizing text\n",
        "        if text_lemmatize:\n",
        "            doc = lemmatize_text(doc)\n",
        "        \n",
        "        # remove special characters    \n",
        "        if special_char_remove:\n",
        "            doc = strip_special_characters(doc)  \n",
        "        \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        \n",
        "        # remove stopwords\n",
        "        if stopword_remove:\n",
        "            doc = strip_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        processed_text.append(doc)\n",
        "        \n",
        "    return processed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oew9jLz-IjME"
      },
      "source": [
        "norm_complete_reviews=clean_text(reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOpVCt_2IpcE"
      },
      "source": [
        "#Sentiment Analysis with SentiWordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkBS78P-ItXb"
      },
      "source": [
        "from nltk.corpus import sentiwordnet as swn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISDS1uFyIwSc",
        "outputId": "e8dc8767-fa61-4278-9903-203239d44f9f"
      },
      "source": [
        "awesome = list(swn.senti_synsets('happy', 'a'))[0]\n",
        "print('Positive Polarity Score:', awesome.pos_score())\n",
        "print('Negative Polarity Score:', awesome.neg_score())\n",
        "print('Objective Score:', awesome.obj_score())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Polarity Score: 0.875\n",
            "Negative Polarity Score: 0.0\n",
            "Objective Score: 0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXy_te7aIzqR"
      },
      "source": [
        "def analyze_sentiment_sentiwordnet_lexicon(review,verbose=False):\n",
        "\n",
        "    # tokenize and POS tag text tokens\n",
        "    tokens = nltk.word_tokenize (review)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    tagged_text = nltk.pos_tag(tokens)\n",
        "    #tagged_text = [(token.text, token.tag_) for token in tokenized_text]\n",
        "    pos_score = neg_score = token_count = obj_score = 0\n",
        "    # get wordnet synsets based on POS tags\n",
        "    # get sentiment scores if synsets are found\n",
        "    for word, tag in tagged_text:\n",
        "        ss_set = None\n",
        "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
        "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
        "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
        "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
        "        # if senti-synset is found        \n",
        "        if ss_set:\n",
        "            # add scores for all found synsets\n",
        "            pos_score += ss_set.pos_score()\n",
        "            neg_score += ss_set.neg_score()\n",
        "            obj_score += ss_set.obj_score()\n",
        "            token_count += 1\n",
        "    \n",
        "    # aggregate final scores\n",
        "    final_score = pos_score - neg_score\n",
        "    norm_final_score = round(float(final_score) / token_count, 2)\n",
        "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
        "    if verbose:\n",
        "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
        "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
        "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
        "        # to display results in a nice table\n",
        "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, norm_neg_score, norm_final_score]],\n",
        "                                       columns=['Predicted Sentiment', 'Objectivity', 'Positive', 'Negative', 'Overall'])\n",
        "        print(sentiment_frame)\n",
        "        \n",
        "    return final_sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGXKYGn-LyaM"
      },
      "source": [
        "Predict Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shIlkepSLzfc"
      },
      "source": [
        "sample_id= [4726,12103,25726,49255]\n",
        "sample_reviews= reviews[sample_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "VsFHtLtsL4c7",
        "outputId": "d3eba4ab-670f-41b5-ccc8-5692109c9c97"
      },
      "source": [
        "sample_reviews[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What the (beep) is going wrong with Disney the last years? Are there totally run out of good ideas? Where is the magic? Where are the good animators, the good songwriters, the good directors, the good... Okay, i know, Walt himself and the famous \"nine old man\" can\\'t come back. But is this a reason to crank out countless of those cheap sequels and slowly but surely destroying the ideals of Walt Disney? I never rent or bought a Disney-sequel of what movie however. Because i had read much enough about its (absence of) quality. But \"Atlantis: Milo\\'s Return\" was aired today on TV in Germany and so i watch it. It confirmed my doubts about sequels. It was absolutely boring. Flaw animation, primitive color-rotation, simple characters, some unsuccessful tries to simulate the famous Multiplane-Camera with CGI, mediocre music and a patchwork of different, simple stories. It looks absolutely not like Disney! Not like Disney i know! It looks like one of the countless, cheap and simple animation-series like \"DragonballZ\", \"Beyblade\" etc. that aired every day on TV for children.<br /><br />My first reaction after showing this crap, was to load \"Bambi\" in my DVD-Player, to see Disney\\'s immortal magic, depth, spirit and charm again, to see Disney on its climax again, to see the awesome art of handmade animation again. \"Bambi\" was the first (and until today the only) movie that i give 10 out of 10 stars. But \"Atlantis: Milo\\'s Return\"? No magic, no depth, no charm, no spirit... It deserved only 3 out of 10!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5md0EVIL7q6"
      },
      "source": [
        "norm_reviews=clean_text(sample_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "cKmv04ecMAS5",
        "outputId": "cbdd9959-2a92-4b5c-bcde-a6155990a10f"
      },
      "source": [
        "norm_reviews[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'beep going wrong disney last year totally run good idea magic good animator good songwriter good director good okay know walt famous `` nine old man not come back reason crank countless cheap sequel slowly surely destroying ideal walt disney never rent bought disneysequel movie however read much enough absence quality `` atlantis milo return wa aired today tv germany watch confirmed doubt sequel wa absolutely boring flaw animation primitive colorrotation simple character unsuccessful try simulate famous multiplanecamera cgi mediocre music patchwork different simple story look absolutely not like disney not like disney know look like one countless cheap simple animationseries like `` dragonballz `` beyblade etc aired every day tv child first reaction showing crap wa load `` bambi dvdplayer see disney immortal magic depth spirit charm see disney climax see awesome art handmade animation `` bambi wa first today movie give 10 10 star `` atlantis milo return no magic no depth no charm no spirit deserved 3 10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pqb7KsWJT7Y",
        "outputId": "e3405345-d4e9-4d17-e978-60bb1759ecfa"
      },
      "source": [
        "for review, sentiment in zip(reviews[sample_id], sentiments[sample_id]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True) \n",
        "    print('*'*40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: Utter dreck. I got to the 16 minute/27 second point, and gave up. I'd have given it a negative number review if that were possible (although 'pissible' is a more fitting word...). Unlike the sizzle you could see and practically feel between MacMurray and Stanwyck in the original, the chemistry between dumb ol' Dicky Crenna and whats-her-face here is just non-existent. The anklet becomes an unattractive chunky bracelet? There's no ciggy-lighting-by-fingertip? And I thought I'd be SICK when they have a mortified-looking (and rightly so, believe you me) Lee J. Cobb as Keyes practically burping/upchucking his way through the explanation of his \"Little Man\" to Mr. Garloupis. No offence to the non-sighted, but it looks as though a posse of blind men ran amuck with the set design of both the Dietrichson and Neff houses. The same goes for those horrid plaid pants that Phyllis wears. And crikey, how much $$ does Neff make, that he lives overlooking a huge marina? This, folks, again, all takes place in the first 16 and a half minutes. If you can get through more of it, you have a much stronger constitution than me, or you are a masochist. But please, take some Alka-Seltzer first, or you WILL develop a \"little man\" of your own that may never go away. Proceed with caution, obviously.\n",
            "Actual Sentiment: negative\n",
            "  Predicted Sentiment  Objectivity  Positive  Negative  Overall\n",
            "0            negative         0.86      0.06      0.07    -0.01\n",
            "****************************************\n",
            "REVIEW: This must be the worst thriller I have seen in a long long time. The directing, the acting and the adaptation of the story leave what could probably have been a good plot into a meaningless waste of time. Within a few minutes of watching the film it was easy to figure out the whole plot and then there are more obvious clues very early on leaving no mystery. I guessed this within the first few minutes and I kept hoping I was wrong and much to my dismay I was not.<br /><br />The film starts off with two FBI agents who drive to a remote town to investigate a murderous spree which has left three witnesses, a young girl, a drug addict and a cop. They are interviewed under surveillance cameras separately and each tells their account of the day. Each has something to hide about themselves and the day unfolds as they tell their accounts. This part is probably the saving grace and if developed could have made this film better.<br /><br />Spoiler: The whole story ends in the FBI agents being the actual killers and the young girl is the only one who has figured this out and so left unhurt by them.<br /><br />Why do they go through the whole charade of interviewing three witnesses and bonding with the young girl if their idea had been to kill them in the first place? How did they get away with pretending to be FBI agents (when you discover that real FBI agents had been killed and their badges were found on them)? How did they know how to set up and use the surveillance cameras?<br /><br />Bill Pullman and Julia Ormond are so unconvincing from the beginning to the end. Maybe the idea is to develop their characters for the revelation at the end. Come on, they both look ridiculous, stupid and not sinister in the least. The character of the young girl is also wasted potential. There is no meaning to her actions and no meaning to whom she prefers to bond with in her ordeal. She does not appear distressed, but rather detached which again is not explained. <br /><br />Awful film on the whole.\n",
            "Actual Sentiment: negative\n",
            "  Predicted Sentiment  Objectivity  Positive  Negative  Overall\n",
            "0            positive         0.85      0.08      0.07     0.01\n",
            "****************************************\n",
            "REVIEW: What the (beep) is going wrong with Disney the last years? Are there totally run out of good ideas? Where is the magic? Where are the good animators, the good songwriters, the good directors, the good... Okay, i know, Walt himself and the famous \"nine old man\" can't come back. But is this a reason to crank out countless of those cheap sequels and slowly but surely destroying the ideals of Walt Disney? I never rent or bought a Disney-sequel of what movie however. Because i had read much enough about its (absence of) quality. But \"Atlantis: Milo's Return\" was aired today on TV in Germany and so i watch it. It confirmed my doubts about sequels. It was absolutely boring. Flaw animation, primitive color-rotation, simple characters, some unsuccessful tries to simulate the famous Multiplane-Camera with CGI, mediocre music and a patchwork of different, simple stories. It looks absolutely not like Disney! Not like Disney i know! It looks like one of the countless, cheap and simple animation-series like \"DragonballZ\", \"Beyblade\" etc. that aired every day on TV for children.<br /><br />My first reaction after showing this crap, was to load \"Bambi\" in my DVD-Player, to see Disney's immortal magic, depth, spirit and charm again, to see Disney on its climax again, to see the awesome art of handmade animation again. \"Bambi\" was the first (and until today the only) movie that i give 10 out of 10 stars. But \"Atlantis: Milo's Return\"? No magic, no depth, no charm, no spirit... It deserved only 3 out of 10!\n",
            "Actual Sentiment: negative\n",
            "  Predicted Sentiment  Objectivity  Positive  Negative  Overall\n",
            "0            positive         0.82      0.11      0.07     0.04\n",
            "****************************************\n",
            "REVIEW: Assassin Hauser's (John Cusak) mission is to whack a Mid-Eastern oil minister, whose name happens to be Omar Sharif (Neikov), in the country of Turaqistan which is run by American interests. Hauser poses as Trade Show producer to allow him to get to Omar.<br /><br />Sometimes a satire can be so overdone it becomes most annoying. Here it does too much: the government, politics, music, war, people not generally accepted by society, and did I mention \"war.\" And, that is what we have here - a most annoying movie that borders on a very bad nightmare brought to life. I am still asking myself why I continued with the DVD. Also, there are so many Cusak family members in this that John Cusak appears embarrassed by the family just being there, or is that just me?<br /><br />It used to be that a John Cusak movie, while a little offbeat, was, in the end, rather good. Not here. Believe that John Cusak had a hand in the writing and producing of this mess. Make of that what you will.<br /><br />There is too much going on in the movie accompanied by constant gun-fire, bombings, and shouting that you really cannot focus or was that the point? Probably. It just takes too long to set up the hit, which is largely forgotten until the last 15-minutes. In the meantime we have meaningless banter among all in the cast. And, chemistry between John Cusak and Marisa Tormei? I don't think so, but you know: the boy  girl thing and they needed something to take up more time. <br /><br />Yes, for what they were supposed to be, (offbeat and annoying) the performances of Duff, and Kingsley were good. But, when I saw Dan Aykroyd's character, in the beginning of the show, sitting on a toilet taking a dump, I knew the rest of the show would go to the tank as well. I was not wrong. I am sure some will sing praises of this effort, but if a rose is still a rose by any other name so, too, is a mess<br /><br />I now remember why I continued with the DVD. I was hoping that the story would somehow level out and save itself. Never did.<br /><br />Violence: Yes. Sex: No. Nudity: No. Language: Yes.\n",
            "Actual Sentiment: negative\n",
            "  Predicted Sentiment  Objectivity  Positive  Negative  Overall\n",
            "0            positive         0.85      0.08      0.07      0.0\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbG_4_IMM29O"
      },
      "source": [
        "# Predict Sentiment for Complete Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co9tOdYCJcLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fcf6e8-638f-487f-e751-c20f4aa0cc0c"
      },
      "source": [
        "%%time\n",
        "predicted_sentiments_swn = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_complete_reviews]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 16min 13s, sys: 5.09 s, total: 16min 18s\n",
            "Wall time: 16min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2VQkK6-M8MS"
      },
      "source": [
        "#Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7V_4renP4Lt"
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmM1LSWfGZUa"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq3j0-OQP5eO"
      },
      "source": [
        "def display_metric(actual_sent, predicted_sent):\n",
        "    print(\"Accuracy:\", np.round(metrics.accuracy_score(actual_sent,predicted_sent),4))\n",
        "    print('Precision:', np.round(metrics.precision_score(actual_sent, predicted_sent, average='weighted'),4))\n",
        "    print('Recall:', np.round(metrics.recall_score(actual_sent,predicted_sent,average='weighted'),4))\n",
        "    print('F1 Score:', np.round( metrics.f1_score(actual_sent,predicted_sent,average='weighted'),4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsM3qlSGaiGR"
      },
      "source": [
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrbf7f4vP8An"
      },
      "source": [
        "def display_confusion_matrix(actual_sent, predicted_sent, classes=[1,0]):\n",
        "  # Confusion Matrix\n",
        "\n",
        "  total_classes = len(classes)\n",
        "  level_labels = [total_classes*[0], list(range(total_classes))]\n",
        "\n",
        "  # true_pos = (actual_sent == 1) & (predicted_sent == 1)\n",
        "  # false_pos = (actual_sent == 0) & (predicted_sent == 1)\n",
        "  # false_neg = (actual_sent == 1) & (predicted_sent == 0)\n",
        "  # true_neg = (actual_sent == 0) & (predicted_sent == 0)\n",
        "  \n",
        "  cm = metrics.confusion_matrix(y_true=actual_sent, y_pred=predicted_sent, labels=classes)\n",
        "  confusion_matrix1 = pd.DataFrame(data=cm,columns=pd.MultiIndex(levels=[['predicted:'], \n",
        "                                                    ['positive', 'negative']], \n",
        "                                                    codes=[[0,0],[0,1]]),\n",
        "                         index = pd.MultiIndex(levels=[['Actual:'], \n",
        "                                                    ['positive', 'negative']], \n",
        "                                                    codes=[[0,0],[0,1]]))\n",
        "  print(confusion_matrix1)\n",
        "\n",
        "\n",
        "\n",
        "    # total_classes = len(classes)\n",
        "    # level_labels = [total_classes*[0], list(range(total_classes))]\n",
        "\n",
        "    # cm = metrics.confusion_matrix(y_true=actual_sent, y_pred=predicted_sent, labels=classes)\n",
        "    # cm_frame = pd.DataFrame(data=cm, columns=pd.MultiIndex(levels=[['Predicted:'], classes], labels=level_labels),\n",
        "    #                         index=pd.MultiIndex(levels=[['Actual:'], classes], labels=level_labels)) \n",
        "    # print(cm_frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnjtxtg0QAEX"
      },
      "source": [
        "def display_classification_report(actual_sent, predicted_sent, classes=['positive', 'negative']):\n",
        "\n",
        "    report = metrics.classification_report(y_true=actual_sent,y_pred=predicted_sent, labels=classes) \n",
        "    print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF9NiKhlQKYy"
      },
      "source": [
        "def display_model_performance(actual_sent, predicted_sent, classes=[1,0]):\n",
        "    print('Model Performance metrics:')\n",
        "    print('*'*50)\n",
        "    display_metric(actual_sent=actual_sent, predicted_sent=predicted_sent)\n",
        "    print('\\n')\n",
        "    print('\\nModel Classification report:')\n",
        "    print('*'*50)\n",
        "    display_classification_report(actual_sent=actual_sent, predicted_sent=predicted_sent,classes=classes)  \n",
        "    print('\\n')    \n",
        "    print('\\nPrediction Confusion Matrix:')\n",
        "    print('*'*50)\n",
        "    display_confusion_matrix(actual_sent=actual_sent, predicted_sent=predicted_sent,classes=classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxdAtYjZM7sF",
        "outputId": "d98f0a61-8484-4415-c068-a9288ab22ddf"
      },
      "source": [
        "display_model_performance(actual_sent=sentiments, predicted_sent=predicted_sentiments_swn,classes=['positive', 'negative'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "**************************************************\n",
            "Accuracy: 0.6772\n",
            "Precision: 0.6824\n",
            "Recall: 0.6772\n",
            "F1 Score: 0.6749\n",
            "\n",
            "\n",
            "\n",
            "Model Classification report:\n",
            "**************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.65      0.76      0.70     25000\n",
            "    negative       0.71      0.59      0.65     25000\n",
            "\n",
            "    accuracy                           0.68     50000\n",
            "   macro avg       0.68      0.68      0.67     50000\n",
            "weighted avg       0.68      0.68      0.67     50000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "**************************************************\n",
            "                 predicted:         \n",
            "                   positive negative\n",
            "Actual: positive      19050     5950\n",
            "        negative      10190    14810\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}